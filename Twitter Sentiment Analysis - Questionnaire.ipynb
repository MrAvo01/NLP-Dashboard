{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\avanoostveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\avanoostveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\avanoostveen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "extra_stopwords = (\"rt\", \"url\", \"ul\")\n",
    "\n",
    "for word in extra_stopwords:\n",
    "    stopwords_set.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Class: Scrape, Clean, Tokenize and Lemmatize Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from tweepy import OAuthHandler\n",
    "class TwitterMain(object): \n",
    "    def __init__(self):\n",
    "        # Access Credentials \n",
    "        consumer_key = 'qomm8PtOchZtPv3u0Qf7itE4w'\n",
    "        consumer_secret = 'YQya6zGND5ULBePviyKWxIFRh19vqaMyhIp0f67r23aEmcLqPs'\n",
    "        access_token = '958414889646788609-QKHp30udwA1hlzNxp3KAvxTFWwGgZoF'\n",
    "        access_token_secret = '2GcwRYoWRs6vQL8DO9cxxkydPEpRjNJ1OZSznBuTFfD5U'\n",
    "        \n",
    "        try: \n",
    "            # OAuthHandler object \n",
    "            auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(auth, wait_on_rate_limit=False, wait_on_rate_limit_notify=True)\n",
    "            \n",
    "        except tweepy.TweepError as e:\n",
    "            print(f\"Error: Twitter Authentication Failed - \\n{str(e)}\") \n",
    "\n",
    "    # Function to fetch tweets\n",
    "    def get_tweets(self, query, maxTweets = 1000): \n",
    "        # empty list to store parsed tweets \n",
    "        sleep_on_rate_limit=False\n",
    "        tweets = [] \n",
    "        sinceId = None\n",
    "        max_id = -1\n",
    "        tweetCount = 0\n",
    "        tweetsPerQry = 100\n",
    "        \n",
    "        for word in query.split():\n",
    "            stopwords_set.add(word.lower())\n",
    "        \n",
    "        while tweetCount < maxTweets:\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry, tweet_mode='extended')\n",
    "                    else:\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry, tweet_mode='extended',\n",
    "                                                since_id=sinceId)\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry, tweet_mode='extended',\n",
    "                                                max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = self.api.search(q=query, count=tweetsPerQry, tweet_mode='extended',\n",
    "                                                max_id=str(max_id - 1),\n",
    "                                                since_id=sinceId)\n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "                    \n",
    "                for tweet in new_tweets:\n",
    "                    if tweet.lang == 'en':\n",
    "                        parsed_tweet = {} \n",
    "                        parsed_tweet['tweets'] = tweet.full_text\n",
    "\n",
    "                        # appending parsed tweet to tweets list \n",
    "                        if tweet.full_text.startswith(\"RT @\"):\n",
    "                            parsed_tweet['tweets'] = tweet.retweeted_status.full_text\n",
    "                            \n",
    "                            # if tweet has retweets, ensure that it is appended only once \n",
    "                            if parsed_tweet not in tweets: \n",
    "                                tweets.append(parsed_tweet) \n",
    "                        else: \n",
    "                            tweets.append(parsed_tweet) \n",
    "                        \n",
    "                        tweetCount = len(tweets)\n",
    "                        max_id = new_tweets[-1].id\n",
    "\n",
    "            except tweepy.TweepError as e:\n",
    "                print(\"Tweepy error : \" + str(e))\n",
    "                break\n",
    "        \n",
    "        return pd.DataFrame(tweets)\n",
    "    \n",
    "    # Removes punctuation marks from textual input\n",
    "    def remove_pattern(text, pattern_regex):\n",
    "        r = re.findall(pattern_regex, text)\n",
    "        for i in r:\n",
    "            text = re.sub(i, '', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    # Removes emojis from textual input\n",
    "    def remove_emoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        \n",
    "        return emoji_pattern.sub(r'',text)\n",
    "\n",
    "    # Removes url from textual input\n",
    "    def remove_URL(text):\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        \n",
    "        return url.sub(r'URL',text)\n",
    "    \n",
    "    # Removes unwanted tokens from Tweets\n",
    "    def clean_tweets(tweets_df):\n",
    "\n",
    "        cleaned_tweets = []\n",
    "\n",
    "        tweets_df['tweets'] = np.vectorize(TwitterMain.remove_pattern)(tweets_df['tweets'], \"\\)\\(.[\\w]*: | *RT*\")\n",
    "        tweets_df['tweets'] = np.vectorize(TwitterMain.remove_emoji)(tweets_df['tweets'])\n",
    "        tweets_df['tweets'] = np.vectorize(TwitterMain.remove_URL)(tweets_df['tweets'])\n",
    "\n",
    "        for index, row in tweets_df.iterrows():\n",
    "            x = [word for word in row.tweets.split() if \"@\" not in word and word not in stopwords_set]\n",
    "            cleaned_tweets.append(' '.join(x))\n",
    "\n",
    "        return cleaned_tweets\n",
    "\n",
    "    # Tokenizes each Tweet\n",
    "    def tokenize(tweets_df):\n",
    "        # Tokenization\n",
    "        tokenized_tweets = tweets_df['tweets'].apply(lambda x: x.split())\n",
    "\n",
    "        # Finding Lemma for each word\n",
    "        word_lemmatizer = WordNetLemmatizer()\n",
    "        tokenized_tweets = tokenized_tweets.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\n",
    "\n",
    "        #joining words into sentences (from where they came from)\n",
    "        for i, tokens in enumerate(tokenized_tweets):\n",
    "            tokenized_tweets[i] = ' '.join(tokens)\n",
    "\n",
    "        return tokenized_tweets\n",
    "    \n",
    "    # Performs sentiment analysis over given textual input\n",
    "    def sentiment(text):\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        polarity_scores = sid.polarity_scores(text)\n",
    "        \n",
    "        if polarity_scores['compound'] == 0:\n",
    "            return 'Neutral', polarity_scores['compound']\n",
    "        elif polarity_scores['neg'] > polarity_scores['pos']:\n",
    "            return 'Negative', polarity_scores['compound']\n",
    "        else:\n",
    "            return 'Positive', polarity_scores['compound']\n",
    "    \n",
    "    # Returns a list of the frequencies per word in a search query\n",
    "    def frequencies(df):\n",
    "        all_words = []\n",
    "        for line in list(df['tweets']):\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                all_words.append(word.lower())\n",
    "\n",
    "        # create a word frequency dictionary\n",
    "        return Counter(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more tweets found\n",
      "No more tweets found\n",
      "No more tweets found\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [28/Jan/2021 10:47:31] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Jan/2021 10:47:33] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Jan/2021 10:47:33] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [28/Jan/2021 10:47:33] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "###############################################----IMPORTS & SETUP---###############################################\n",
    "\n",
    "\n",
    "import dash\n",
    "import dash_table\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Use external stylesheet from github for layout\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "# Create application\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "\n",
    "###################################################----METHODS---###################################################\n",
    "\n",
    "\n",
    "# Scrapes Twitter and performs sentiment analysis\n",
    "def fetch_Tweets(value):\n",
    "    \n",
    "    # Fetch tweets based on search query (value)\n",
    "    dash_tweets = TwitterMain.get_tweets(TwitterMain(), str(value))\n",
    "    \n",
    "    # Store full Tweets in a separate column\n",
    "    dash_tweets['Full Tweet']=dash_tweets['tweets']\n",
    "    \n",
    "    # Make sure all Tweets are string types\n",
    "    dash_tweets['tweets']=dash_tweets['tweets'].apply(str)\n",
    "\n",
    "    # Clean Tweets (Remove stopwords, punctuation marks, usernames, emojis and url's)\n",
    "    dash_tweets['tweets'] = dash_tweets['tweets'].str.replace(\"[^a-zA-Z\\s]\", \"\")\n",
    "    dash_tweets['tweets'] = TwitterMain.clean_tweets(dash_tweets)\n",
    "\n",
    "    # Tokenize and Lemmatize Tweets\n",
    "    dash_tweets['tweets'] = TwitterMain.tokenize(dash_tweets)\n",
    "    \n",
    "    # Make sentiment (pos-neg-neutral) and sentiment score (-1 to 1) columns\n",
    "    dash_tweets[\"Sentiment\"]=\"\"\n",
    "    dash_tweets[\"Sentiment Score\"]=\"\"\n",
    "\n",
    "    counter = 0\n",
    "    # Perform sentiment analysis and fill columns with sentiment (pos-neg-neutral) and sentiment score (-1 to 1)\n",
    "    for text in dash_tweets[\"tweets\"]:\n",
    "        dash_tweets[\"Sentiment\"][counter], dash_tweets[\"Sentiment Score\"][counter] = TwitterMain.sentiment(text)\n",
    "        counter +=1\n",
    "    \n",
    "    # Derive frequency of each word and transform into a sorted dataframe\n",
    "    wordfreq = TwitterMain.frequencies(dash_tweets)\n",
    "    df_wf = pd.DataFrame.from_dict(wordfreq, orient='index').reset_index()\n",
    "    df_wf.columns = ['Word', 'Frequency']\n",
    "    df_wf = df_wf.sort_values(by = \"Frequency\", ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    # Create separate dataframe containing top 25 most positive and most negative tweets\n",
    "    top_tweets_df = pd.DataFrame()\n",
    "    top_tweets_df[\"Positive\"] = dash_tweets.sort_values(by = \"Sentiment Score\", ascending = False)[\"Full Tweet\"][:25].reset_index(drop = True)\n",
    "    top_tweets_df[\"Negative\"] = dash_tweets.sort_values(by = \"Sentiment Score\")[\"Full Tweet\"][:25].reset_index(drop=True)\n",
    "    \n",
    "    return dash_tweets, df_wf, top_tweets_df    \n",
    "\n",
    "# Dictionary containing the values in the python dropdown menu\n",
    "options_drop=[\n",
    "#             {'label': 'KPMG', 'value': 'KPMG_NL'},\n",
    "            {'label': 'ING', 'value': 'ING_BANK'},\n",
    "            {'label': 'Shell', 'value': 'SHELL_Nederland'},\n",
    "#             {'label': 'Aegon', 'value': 'AEGON'}\n",
    "#             {'label': 'Akzo Nobel', 'value': '#AKZO'},\n",
    "#             {'label': 'ASML', 'value': '#ASML'}\n",
    "            {'label': 'Adyen', 'value': 'ADYEN'}\n",
    "#             {'label': 'ASR', 'value': '#ASRNL'},\n",
    "#             {'label': 'Heineken', 'value': '#HEIA'}\n",
    "#             {'label': 'Ahold', 'value': '#Ahold'},\n",
    "#             {'label': 'Philips', 'value': 'Philips'}\n",
    "#             {'label': 'Nationale Nederlanden', 'value': '#NN'},\n",
    "#             {'label': 'Unilever', 'value': 'Unilever'}\n",
    "        ]\n",
    "\n",
    "# Create dictionaries to store Tweets, wordfrequencies and top 25 most positive and negative Tweets per search query\n",
    "tweets = {}\n",
    "tweets_wf = {}\n",
    "top_tweets = {}\n",
    "\n",
    "# Fill the dictionaries with Tweets, wordfrequencies and top 25 most positive and negative Tweets per search query\n",
    "for company in options_drop:\n",
    "    tweets[company[\"value\"]], tweets_wf[company[\"value\"]], top_tweets[company[\"value\"]] = fetch_Tweets(company[\"value\"])\n",
    "\n",
    "    \n",
    "#############################################----DASHBOARD LAYOUT---#############################################\n",
    "\n",
    "\n",
    "app.layout = html.Div(children=[\n",
    "    # Title\n",
    "    html.H1(children='Twitter Sentiment Analysis'),\n",
    "    \n",
    "    # Introduction text\n",
    "    dcc.Markdown('''\n",
    "        In this questionnaire, the usability of a tool will be put to the test.\n",
    "        \n",
    "        The functionalities of this tool are the following:\n",
    "         - It scrapes Twitter for Tweets containing a specific search query.\n",
    "         - It performs a Sentiment Analysis over these Tweets. \n",
    "         - It returns a pie chart containing the number of positive, negative and neutral Tweets that were returned.\n",
    "         - It returns a histogram containing the word frequency of the most common words.\n",
    "         - It returns the top positive and negative Tweets per search query.\n",
    "         \n",
    "        The goal of this tool is to support auditors in assessing the risk profile around a company. Specifically, this\n",
    "        tool is designed to help identify impairment triggers, based on the sentiment found in Tweets about a certain \n",
    "        company.\n",
    "        \n",
    "        To test whether this tool could be useful for auditors in the process of identifying impairment triggers, it is \n",
    "        asked of you, as an expert in this field, to assess the applicability of the information returned by this tool in\n",
    "        the identification process of impairment triggers. This will be done through means of a questionnaire, that can help\n",
    "        determine how well this tool aids in identifying impairment triggers. Each of the questions contains an aspect of\n",
    "        the tool\n",
    "        \n",
    "        In order to validate the usability of this tool with regards to certain functionalities it is asked of you to rate \n",
    "        the following applications on a scale of 1 to 10, with 1 indicating not usable and 10 indicating extremely useful.\n",
    "        Please repeat this process for all companies in the dropdown list.\n",
    "        \n",
    "        Furthermore, for each of the spplications that you are asked to rate, it is asked of you to leave a comment, \n",
    "        indicating what you think this tool is missing and what aspect of this tool is most useful.\n",
    "    '''),\n",
    "    \n",
    "    # Subtitle for dropdown menu\n",
    "    html.H2(children='Select a company:'),\n",
    "    \n",
    "    # Dropdown menu\n",
    "    dcc.Dropdown(\n",
    "        id='fig_dropdown',\n",
    "        options=options_drop,\n",
    "        value = \"ING_BANK\"\n",
    "    ),\n",
    "    \n",
    "    # Subtitle for pie chart\n",
    "    html.H2(children='Pie chart: positive, negative and neutral Tweets'),\n",
    "    \n",
    "    # Pie chart\n",
    "    dcc.Graph(id=\"pie-chart\"),\n",
    "    \n",
    "    # Subtitle for histogram\n",
    "    html.H2(children='Histogram of most common words'),\n",
    "    \n",
    "    # Histogram\n",
    "    dcc.Graph(id='histogram'),\n",
    "    \n",
    "    # Subtitle for \"Top Tweets\"\n",
    "    html.H2(children='Most positive and most negative Tweets'),\n",
    "    \n",
    "    # Data table containing top 25 most positive and most negative Tweets\n",
    "    dash_table.DataTable(\n",
    "        id='table',\n",
    "        style_cell=\n",
    "            {\n",
    "                'textAlign': 'center',\n",
    "                'whiteSpace': 'normal',\n",
    "                'height': 'auto',\n",
    "            },\n",
    "        style_data_conditional=[\n",
    "            {\n",
    "                'if': {'row_index': 'odd'},\n",
    "                'backgroundColor': 'rgb(248, 248, 248)'\n",
    "            }\n",
    "        ],\n",
    "        style_header={\n",
    "            'backgroundColor': 'rgb(230, 230, 230)',\n",
    "            'fontWeight': 'bold'\n",
    "        },\n",
    "        data = []\n",
    "    )\n",
    "\n",
    "],\n",
    "        # Styling of dashboard\n",
    "        style={'marginLeft': 25, 'marginRight': 25, 'marginTop': 25, 'marginBottom': 25,\n",
    "           'backgroundColor':'#F7FBFE',\n",
    "           'border': 'thin lightgrey', \n",
    "           'padding': '10px 5px 10px 5px'\n",
    "})\n",
    "\n",
    "\n",
    "#########################################----DASHBOARD INPUT & OUTPUT---#########################################\n",
    "\n",
    "\n",
    "@app.callback([\n",
    "    # Returns a pie chart using user input from the dashboard\n",
    "    dash.dependencies.Output(component_id = 'pie-chart', component_property = 'figure'),\n",
    "    \n",
    "    # Returns a histogram using user input from the dashboard\n",
    "    dash.dependencies.Output(component_id = 'histogram', component_property = 'figure'),\n",
    "    \n",
    "    # Returns data for the data table using user input from the dashboard\n",
    "    dash.dependencies.Output(component_id = 'table', component_property = 'data'),\n",
    "    \n",
    "    # Returns columns for the data table using user input from the dashboard\n",
    "    dash.dependencies.Output(component_id = 'table', component_property = 'columns')],\n",
    "    \n",
    "    # Fetches user input from the dropdown component from the dashboard to use in the function below\n",
    "    dash.dependencies.Input(component_id = 'fig_dropdown',  component_property = 'value'))\n",
    "\n",
    "# Uses the user input from the dashboard and returns the output as described above\n",
    "def update_dashboard(value):\n",
    "\n",
    "    # Creates pie chart based on user input\n",
    "    pie = px.pie(tweets[value], values = tweets[value][\"Sentiment\"].value_counts(), \n",
    "                        names = tweets[value][\"Sentiment\"].unique(),\n",
    "                        color = tweets[value][\"Sentiment\"].unique(),\n",
    "                        color_discrete_map={'Positive':'royalblue',\n",
    "                                            'Negative':'#ff0000',\n",
    "                                            'Neutral':'#bbc4cc'})\n",
    "    \n",
    "    # Creates histogram based on user input\n",
    "    hist = px.histogram(tweets_wf[value][1:26], x=\"Word\", y = \"Frequency\", title = \"Word Frequency\", nbins = 20)\n",
    "    \n",
    "    # Creates a dictionary containing data for the data table based on user input\n",
    "    data = top_tweets[value].to_dict(\"records\")\n",
    "    \n",
    "    # Creates a list of columns based on user input\n",
    "    columns = [{\"name\": i, \"id\": i} for i in top_tweets[value].columns]\n",
    "    \n",
    "    # Return the pie chart, histogram, data and columns\n",
    "    return pie, hist, data, columns\n",
    "\n",
    "\n",
    "#########################################----RUN APPLICATION---#########################################\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
